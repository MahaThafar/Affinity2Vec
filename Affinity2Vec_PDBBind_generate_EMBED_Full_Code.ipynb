{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import math as math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time, os, sys, json,pickle\n",
    "from math import exp\n",
    "import itertools\n",
    "from smart_open import open\n",
    "\n",
    "# Similarity and normalization packages\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.preprocessing import MinMaxScaler, minmax_scale\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import scipy.spatial\n",
    "# Machine learninig packages\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model, ensemble, metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# for protein embedding\n",
    "import biovec\n",
    "\n",
    "# for drugs embedding\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# deepchem library\n",
    "import deepchem as dc\n",
    "from deepchem.feat import Featurizer\n",
    "from deepchem.models.optimizers import ExponentialDecay, Adam\n",
    "from deepchem.metrics import to_one_hot\n",
    "from deepchem.trans import undo_transforms\n",
    "from deepchem.models.graph_models import GraphConvModel, L2Loss,Dense,  Reshape,Dropout \n",
    "import networkx\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Import my files\n",
    "from training_functions import *\n",
    "from pathScores_functions import *\n",
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LigandID</th>\n",
       "      <th>pdbID</th>\n",
       "      <th>Affinity</th>\n",
       "      <th>Year</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLY</td>\n",
       "      <td>2r58</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2007</td>\n",
       "      <td>C[NH+](C)CCCCC([NH3+])C(=O)O</td>\n",
       "      <td>AFDWDAYLEETGSEAAPAKCFKQAQNPPNNDFKIGMKLEALDPRNV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRP</td>\n",
       "      <td>3c2f</td>\n",
       "      <td>1.995679</td>\n",
       "      <td>2008</td>\n",
       "      <td>O=P(O)(O)OCC1OC(OP(=O)(O)OP(=O)(O)O)C(O)C1O</td>\n",
       "      <td>PVYEHLLPVNGAWRQDVTNWLSEDVPSFDFGGYVVGSDLKEANLYC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3HP</td>\n",
       "      <td>3pce</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1998</td>\n",
       "      <td>O=C(O)Cc1cccc(O)c1</td>\n",
       "      <td>PIELLPETPSQTAGPYVHIGLALEAAGNPTRDQEIWNRLAKPDAPG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TDR</td>\n",
       "      <td>4qsu</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2014</td>\n",
       "      <td>CC1C=NC(=O)NC1=O</td>\n",
       "      <td>SMQEEDTFRELRIFLRNVTHRLAIDKRFRVFTKPVDPDEVPDYVTV...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LigandID pdbID  Affinity  Year                                       SMILES  \\\n",
       "0      MLY  2r58  2.000000  2007                 C[NH+](C)CCCCC([NH3+])C(=O)O   \n",
       "1      PRP  3c2f  1.995679  2008  O=P(O)(O)OCC1OC(OP(=O)(O)OP(=O)(O)O)C(O)C1O   \n",
       "2      3HP  3pce  2.000000  1998                           O=C(O)Cc1cccc(O)c1   \n",
       "3      TDR  4qsu  2.000000  2014                             CC1C=NC(=O)NC1=O   \n",
       "\n",
       "                                                 Seq  \n",
       "0  AFDWDAYLEETGSEAAPAKCFKQAQNPPNNDFKIGMKLEALDPRNV...  \n",
       "1  PVYEHLLPVNGAWRQDVTNWLSEDVPSFDFGGYVVGSDLKEANLYC...  \n",
       "2  PIELLPETPSQTAGPYVHIGLALEAAGNPTRDQEIWNRLAKPDAPG...  \n",
       "3  SMQEEDTFRELRIFLRNVTHRLAIDKRFRVFTKPVDPDEVPDYVTV...  "
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "all_DTA_info = pd.read_csv('PDBBind_Refined/All_PDBbind_info.csv')\n",
    "all_DTA_info.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ligand-Protein complexes Preprocessig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2326, 3047)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ligand Preproess\n",
    "DP_aff = all_DTA_info[[\"LigandID\", \"pdbID\",\"Affinity\"]]\n",
    "print(DP_aff.shape)\n",
    "edgeList = DP_aff.values.tolist()\n",
    "edgeList = np.array(edgeList)\n",
    "\n",
    "ligand_no = DP_aff['LigandID'].nunique()\n",
    "protein_no = DP_aff['pdbID'].nunique()\n",
    "\n",
    "allD = DP_aff['LigandID'].unique()\n",
    "allP = DP_aff['pdbID'].unique()\n",
    "\n",
    "# create indexing-dictionaries for drugs (Ligands) and for targets (Proteins)\n",
    "DrID = dict([(d, i) for i, d in enumerate(allD)])\n",
    "PrID = dict([(t, i) for i, t in enumerate(allP)])\n",
    "\n",
    "ligand_no, protein_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3047, 5)\n",
      "(2326, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdbID</th>\n",
       "      <th>LigandID</th>\n",
       "      <th>Affinity</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2r58</td>\n",
       "      <td>MLY</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>C[NH+](C)CCCCC([NH3+])C(=O)O</td>\n",
       "      <td>AFDWDAYLEETGSEAAPAKCFKQAQNPPNNDFKIGMKLEALDPRNV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3c2f</td>\n",
       "      <td>PRP</td>\n",
       "      <td>1.995679</td>\n",
       "      <td>O=P(O)(O)OCC1OC(OP(=O)(O)OP(=O)(O)O)C(O)C1O</td>\n",
       "      <td>PVYEHLLPVNGAWRQDVTNWLSEDVPSFDFGGYVVGSDLKEANLYC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3pce</td>\n",
       "      <td>3HP</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>O=C(O)Cc1cccc(O)c1</td>\n",
       "      <td>PIELLPETPSQTAGPYVHIGLALEAAGNPTRDQEIWNRLAKPDAPG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pdbID LigandID  Affinity                                       SMILES  \\\n",
       "0  2r58      MLY  2.000000                 C[NH+](C)CCCCC([NH3+])C(=O)O   \n",
       "1  3c2f      PRP  1.995679  O=P(O)(O)OCC1OC(OP(=O)(O)OP(=O)(O)O)C(O)C1O   \n",
       "2  3pce      3HP  2.000000                           O=C(O)Cc1cccc(O)c1   \n",
       "\n",
       "                                                 Seq  \n",
       "0  AFDWDAYLEETGSEAAPAKCFKQAQNPPNNDFKIGMKLEALDPRNV...  \n",
       "1  PVYEHLLPVNGAWRQDVTNWLSEDVPSFDFGGYVVGSDLKEANLYC...  \n",
       "2  PIELLPETPSQTAGPYVHIGLALEAAGNPTRDQEIWNRLAKPDAPG...  "
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ligand_un_df = all_DTA_info.copy()\n",
    "print(ligand_un_df.shape)\n",
    "ligand_un_df = ligand_un_df.drop_duplicates(subset=['LigandID'])\n",
    "print(ligand_un_df.shape)\n",
    "ligand_un_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  1.99567863,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  2.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ..., 11.82390874,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "        11.85387196,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        , 11.92081875]])"
      ]
     },
     "execution_count": 900,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DP_Mat,label_row_inds,label_col_inds = edgelist_to_adjMat(edgeList,DrID,PrID)\n",
    "DP_Mat.shape, len(label_row_inds),len(label_col_inds)\n",
    "DP_Mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.35335283e-01, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       [1.00000000e+00, 1.35921383e-01, 1.00000000e+00, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       [1.00000000e+00, 1.00000000e+00, 1.35335283e-01, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       ...,\n",
       "       [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "        7.32726130e-06, 1.00000000e+00, 1.00000000e+00],\n",
       "       [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "        1.00000000e+00, 7.11096951e-06, 1.00000000e+00],\n",
       "       [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "        1.00000000e+00, 1.00000000e+00, 6.65049843e-06]])"
      ]
     },
     "execution_count": 776,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New affinity score for graph G..\n",
    "aff_exp = np.exp((-1)*DP_Mat)\n",
    "aff_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn Embeddings for ligand and proteins\n",
    "## 1) Learn amino-acid sequnce embeddings using ProtVec trined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thafarma/anaconda3/lib/python3.8/site-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import trained model\n",
    "pv = biovec.models.load_protvec('swissprot-reviewed-protvec.model')\n",
    "\n",
    "Embed = {}\n",
    "seqEmbed = []\n",
    "sumEmbed = []\n",
    "avgEmbed = []\n",
    "allEmbed = []\n",
    "\n",
    "for i in range(all_DT_df.shape[0]):\n",
    "    embedding = pv.to_vecs(all_DT_df['Seq'][i])\n",
    "    # Dictionary to get embedding by target ID\n",
    "    Embed[all_DT_df['pdbID'][i]] = embedding\n",
    "    \n",
    "    allem = list(itertools.chain(embedding[0],embedding[1],embedding[2]))\n",
    "    #concatenate 3d\n",
    "    allEmbed.append(allem)\n",
    "    #list of embedding 3D\n",
    "    seqEmbed.append(embedding)\n",
    "    \n",
    "    # 1D embedding by taking sum of 3 ists\n",
    "    sumE = embedding[0]+embedding[1]+embedding[2]\n",
    "    sumEmbed.append(sumE)\n",
    "    \n",
    "    #1D embedding by taking the average\n",
    "    avgEmbed.append(sumE/3)\n",
    "    \n",
    "avgEmbed = np.array(avgEmbed)\n",
    "sumEmbed = np.array(sumEmbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Learn SMILES embeddings using seq2seq fingerprint\n",
    "### a) Prepare SMILES, tokens, and read more smiles data to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read \n",
    "tasks, datasets, transformers = dc.molnet.load_muv()\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "train_smiles = train_dataset.ids\n",
    "valid_smiles = valid_dataset.ids\n",
    "train_smiles_list = list(train_smiles)\n",
    "valid_smiles_list = list(valid_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###.. GET DAVIS SMILES\n",
    "# reading the smiles from the file  (Drugs SMILES)\n",
    "with open('Input/Davis/ligands_can.txt') as f: \n",
    "    dr_data = f.read() \n",
    "# reconstructing the data as a dictionary \n",
    "dr_dv_dic = json.loads(dr_data) \n",
    "dr_dv_keys = list(dr_dv_dic.keys())\n",
    "dr_dv_Sm =  list(dr_dv_dic.values())\n",
    "\n",
    "#---------------------------------------------\n",
    "###.. GET KIBA SMILES\n",
    "# Kiba SMILES\n",
    "with open('Input/Kiba/ligands_can.txt') as f: \n",
    "    dr_kiba = f.read() \n",
    "# reconstructing the data as a dictionary \n",
    "dr_kiba_dic = json.loads(dr_kiba) \n",
    "#print(dr_sm_dic)\n",
    "dr_keys_kiba = list(dr_kiba_dic.keys())\n",
    "dr_kiba_Sm =  list(dr_kiba_dic.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3047, 89004)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr_pdb_Sm = list(all_DT_df['SMILES'])\n",
    "allSM = dr_dv_Sm + dr_kiba_Sm + train_smiles_list + valid_smiles_list + dr_pdb_Sm\n",
    "all_smiles = np.array(allSM)\n",
    "len(dr_pdb_Sm), len(allSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allTokens = set()\n",
    "for s in all_smiles:\n",
    "    allTokens = allTokens.union(set(c for c in s))\n",
    "allTokens = sorted(list(allTokens))\n",
    "\n",
    "max_length = max(len(s) for s in all_smiles)\n",
    "len(allTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b). Creat seq2seq model to learn embeddings for all ligands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Seq2Seq model\n",
    "batch_size = 100\n",
    "model = dc.models.SeqToSeq(allTokens,\n",
    "                           allTokens,\n",
    "                           max_length,\n",
    "                           encoder_layers=2,\n",
    "                           decoder_layers=2,\n",
    "                           variational=True,\n",
    "                           embedding_dimension=128,\n",
    "                           model_dir='fingerprint')\n",
    "\n",
    "\n",
    "\n",
    "batches_per_epoch = len(all_smiles)/batch_size\n",
    "model.optimizer=Adam(learning_rate=ExponentialDecay(0.005, 0.9, batches_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(epochs):\n",
    "    for i in range(epochs):\n",
    "        for s in all_smiles:\n",
    "            yield (s, s)\n",
    "model.fit_sequences(generate_sequences(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2326,)"
      ]
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdb_sm_unique = np.array(list(ligand_un_df['SMILES']))\n",
    "pdb_sm_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3047, 128), (2326, 128), (3047, 100))"
      ]
     },
     "execution_count": 906,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate embeddings\n",
    "pdb_sm_EMBED = model.predict_embeddings(dr_pdb_Sm)\n",
    "pdb_sm_EMBED=np.array(pdb_sm_EMBED)\n",
    "\n",
    "pdb_sm_EMBED_u = model.predict_embeddings(pdb_sm_unique)\n",
    "pdb_sm_EMBED_u =np.array(pdb_sm_EMBED_u)\n",
    "\n",
    "pdb_sm_EMBED.shape, pdb_sm_EMBED_u.shape, sumEmbed.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"trained_seq2seq_SMILES_model.pickle.dat\"\n",
    "# save model to file\n",
    "# pickle.dump(model, open(file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Sequence Embeddings and SMILES Embeddings into file\n",
    "################################################################\n",
    "Pr_EMBED_df = pd.DataFrame.from_dict(sumEmbed)\n",
    "Pr_EMBED_df.to_csv('PDBBind_Refined/EMBED/Pr_ProtVec_EMBED.txt',sep=' ',index=None,header=None)\n",
    "\n",
    "pdb_EMBED_df = pd.DataFrame.from_dict(pdb_sm_EMBED)\n",
    "pdb_EMBED_df.to_csv('PDBBind_Refined/EMBED/Dr_seq2seq_EMBED.txt',sep=' ',index=None,header=None)\n",
    "\n",
    "pdb_EMBED_df_u = pd.DataFrame.from_dict(pdb_sm_EMBED_u)\n",
    "pdb_EMBED_df_u.to_csv('PDBBind_Refined/EMBED/Dr_seq2seq_EMBED_u.txt',sep=' ',index=None,header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data to construct the graph G(V,E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Clculate cosine similarity for ach pair of proteins and each pair of drugs (ligands)\n",
    "\n",
    "DD_Sim_sm = Cosine_Similarity(pdb_EMBED_df)\n",
    "PP_Sim_sq = Cosine_Similarity(sumEmbed)\n",
    "\n",
    "# #normalize simiarities to be in positive range [0,1]\n",
    "DD_Sim_sm = normalizedMatrix(DD_Sim_sm)\n",
    "PP_Sim_sq = normalizedMatrix(PP_Sim_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2326, 2326), (3047, 3047))"
      ]
     },
     "execution_count": 783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pr_SimM = keep_sim_threshold(Pr_SimM ,0.04)\n",
    "# Dr_SimM = keep_sim_threshold(Dr_SimM ,0.35)\n",
    "DD_Sim_sm.shape, PP_Sim_sq.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3047, 2), (3047,))"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XPair = all_DTA_info[[\"LigandID\", \"pdbID\"]]\n",
    "XPair = XPair.to_numpy()\n",
    "\n",
    "Y = np.array(all_DTA_info['Affinity'])\n",
    "\n",
    "XPair.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Create the training, validation and test sets (setting-2)\n",
    "#### (time-base split using the publication years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LigandID</th>\n",
       "      <th>pdbID</th>\n",
       "      <th>Affinity</th>\n",
       "      <th>Year</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLY</td>\n",
       "      <td>2r58</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2007</td>\n",
       "      <td>C[NH+](C)CCCCC([NH3+])C(=O)O</td>\n",
       "      <td>AFDWDAYLEETGSEAAPAKCFKQAQNPPNNDFKIGMKLEALDPRNV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRP</td>\n",
       "      <td>3c2f</td>\n",
       "      <td>1.995679</td>\n",
       "      <td>2008</td>\n",
       "      <td>O=P(O)(O)OCC1OC(OP(=O)(O)OP(=O)(O)O)C(O)C1O</td>\n",
       "      <td>PVYEHLLPVNGAWRQDVTNWLSEDVPSFDFGGYVVGSDLKEANLYC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3HP</td>\n",
       "      <td>3pce</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1998</td>\n",
       "      <td>O=C(O)Cc1cccc(O)c1</td>\n",
       "      <td>PIELLPETPSQTAGPYVHIGLALEAAGNPTRDQEIWNRLAKPDAPG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LigandID pdbID  Affinity  Year                                       SMILES  \\\n",
       "0      MLY  2r58  2.000000  2007                 C[NH+](C)CCCCC([NH3+])C(=O)O   \n",
       "1      PRP  3c2f  1.995679  2008  O=P(O)(O)OCC1OC(OP(=O)(O)OP(=O)(O)O)C(O)C1O   \n",
       "2      3HP  3pce  2.000000  1998                           O=C(O)Cc1cccc(O)c1   \n",
       "\n",
       "                                                 Seq  \n",
       "0  AFDWDAYLEETGSEAAPAKCFKQAQNPPNNDFKIGMKLEALDPRNV...  \n",
       "1  PVYEHLLPVNGAWRQDVTNWLSEDVPSFDFGGYVVGSDLKEANLYC...  \n",
       "2  PIELLPETPSQTAGPYVHIGLALEAAGNPTRDQEIWNRLAKPDAPG...  "
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_DTA_info.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting --2\n",
    "\n",
    "test_index = []\n",
    "train_index = []\n",
    "valid_index = []\n",
    "for i in range (0,all_DTA_info.shape[0]):\n",
    "    if(all_DTA_info['Year'][i] <=2011):\n",
    "        train_index.append(i)\n",
    "   \n",
    "    elif(all_DTA_info['Year'][i]==2012):\n",
    "        valid_index.append(i)\n",
    "                \n",
    "    else:\n",
    "        test_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(547, 312, 2188)"
      ]
     },
     "execution_count": 795,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_index), len(valid_index), len(train_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Affinity2Vec Embed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "FV_targets_sq = np.array(sumEmbed, dtype = float)\n",
    "FV_drugs_sq = np.array(pdb_sm_EMBED, dtype = float)\n",
    "\n",
    "concatenateFV = []\n",
    "class_labels = []\n",
    "lab = []\n",
    "for i in range (all_DTA_info.shape[0]):\n",
    "#     print(i)\n",
    "    features = list(FV_drugs_sq[i]) + list(FV_targets_sq[i])\n",
    "    concatenateFV.append(features)\n",
    "    label = all_DTA_info['Affinity'][i]\n",
    "    lab.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConcatFV shape (3047, 228)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(concatenateFV)\n",
    "YY = np.array(lab)\n",
    "print('ConcatFV shape',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg= xgb.XGBRegressor(booster = 'gbtree', objective ='reg:squarederror', \n",
    "        eval_metric = 'rmse', colsample_bytree = 0.8, learning_rate = 0.01, max_depth = 15,\n",
    "        scale_pos_weight = 1, gamma=0,alpha = 5,n_estimators = 655, tree_method='gpu_hist',\n",
    "        min_child_weight =6,seed=10, gpu_id=1, n_jobs=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(alpha=5, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.8, eval_metric='rmse',\n",
       "             gamma=0, gpu_id=1, importance_type='gain',\n",
       "             interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n",
       "             max_depth=15, min_child_weight=6, missing=nan,\n",
       "             monotone_constraints='(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)',\n",
       "             n_estimators=655, n_jobs=-1, num_parallel_tree=1, random_state=10,\n",
       "             reg_alpha=5, reg_lambda=1, scale_pos_weight=1, seed=10,\n",
       "             subsample=1, tree_method='gpu_hist', validate_parameters=1,\n",
       "             verbosity=None)"
      ]
     },
     "execution_count": 903,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize the feature vector\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train = min_max_scaler.fit(X[train_index])\n",
    "X_train_transform = min_max_scaler.transform(X[train_index])\n",
    "\n",
    "# Train the model using the training sets\n",
    "xg_reg.fit(X_train_transform, Y[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- *EVALUATION using Validation set* -----------------\n",
      "Mean Squared Error: 2.146\n",
      "Concordance index: 0.733445\n",
      "aupr: 0.802\n"
     ]
    }
   ],
   "source": [
    "#######################Validation SET Evaluation######################### \n",
    "X_valid_transform = min_max_scaler.transform(X[valid_index])\n",
    "predictedY= xg_reg.predict(X_valid_transform)\n",
    "\n",
    "print(\"----------------- *EVALUATION using Validation set* -----------------\")\n",
    "# Evaluation Metrics (RMSE, CI, and AUPR)\n",
    "print('Mean Squared Error: %.3f' % mean_squared_error(Y[valid_index], predictedY))\n",
    "print('Concordance index: %3f' % get_cindex(Y[valid_index], predictedY))\n",
    "print('aupr: %.3f' % average_AUPRC(Y[valid_index], predictedY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- *Test Set EVALUATION* -----------------\n",
      "Mean Squared Error: 2.254\n",
      "Concordance index: 0.727442\n",
      "aupr: 0.791\n"
     ]
    }
   ],
   "source": [
    "########################### Test SET Evaluation\n",
    "\n",
    "print(\"----------------- *Test Set EVALUATION* -----------------\")\n",
    "X_test_transform = min_max_scaler.transform(X[test_index])\n",
    "# Make predictions using the testing set\n",
    "predictedY_test= xg_reg.predict(X_test_transform)\n",
    "\n",
    "# Evaluation Metrics (RMSE, CI, and AUPR)\n",
    "print('Mean Squared Error: %.3f' % mean_squared_error(Y[test_index], predictedY_test))\n",
    "print('Concordance index: %3f' % get_cindex(Y[test_index], predictedY_test))\n",
    "print('aupr: %.3f' % average_AUPRC(Y[test_index], predictedY_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Affinity2Vec Path-scors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first thing with affinity train to remove all edges in test set\n",
    "train_aff_M = Mask_test_index(test_index, XPair, aff_exp, DrID, PrID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all featres from the matrix multiplication under each path strucutre..\n",
    "\n",
    "sumDDD, maxDDD = DDD_TTT_sim(DD_Sim_sm)\n",
    "sumTTT, maxTTT= DDD_TTT_sim(PP_Sim_sq)\n",
    "\n",
    "sumDDT,maxDDT = metaPath_Dsim_DT(DD_Sim_sm,train_aff_M,2)\n",
    "sumDTT,maxDTT = metaPath_DT_Tsim(PP_Sim_sq,train_aff_M,2)\n",
    "\n",
    "sumDDDT,_ = metaPath_Dsim_DT(sumDDD,train_aff_M,3)\n",
    "_,maxDDDT = metaPath_Dsim_DT(maxDDD,train_aff_M,3)\n",
    "\n",
    "sumDTTT,_ = metaPath_DT_Tsim(sumTTT,train_aff_M,3)\n",
    "_,maxDTTT = metaPath_DT_Tsim(maxTTT,train_aff_M,3)\n",
    "\n",
    "sumDTDT,maxDTDT = metaPath_DTDT(train_aff_M)\n",
    "sumDDTT,maxDDTT = metaPath_DDTT(train_aff_M,DD_Sim_sm,PP_Sim_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_score = []\n",
    "lab = []\n",
    "for i,j in zip(label_row_inds,label_col_inds): #di , #tj      \n",
    "    pair_scores = (sumDDT[i][j],sumDDDT[i][j],sumDTT[i][j],sumDTTT[i][j], sumDDTT[i][j],\\\n",
    "                   maxDDT[i][j],maxDDDT[i][j],maxDTT[i][j],maxDTTT[i][j],maxDDTT[i][j])\n",
    "    \n",
    "    DT_score.append(pair_scores)\n",
    "    label = aff_exp[i][j]\n",
    "    lab.append(label)\n",
    "\n",
    "FV= np.asarray(DT_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(booster = 'gbtree', objective ='reg:squarederror', eval_metric = 'rmse',\n",
    "                    colsample_bytree = 0.8, learning_rate = 0.01, max_depth = 15, scale_pos_weight = 1, gamma=0,\n",
    "                     alpha = 5,n_estimators = 433, tree_method='gpu_hist',min_child_weight =6, \n",
    "                    seed=10, gpu_id=1, n_jobs=-1) #better & faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(alpha=5, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.8, eval_metric='rmse',\n",
       "             gamma=0, gpu_id=1, importance_type='gain',\n",
       "             interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n",
       "             max_depth=15, min_child_weight=6, missing=nan,\n",
       "             monotone_constraints='(0,0,0,0,0,0,0,0,0,0)', n_estimators=433,\n",
       "             n_jobs=-1, num_parallel_tree=1, random_state=10, reg_alpha=5,\n",
       "             reg_lambda=1, scale_pos_weight=1, seed=10, subsample=1,\n",
       "             tree_method='gpu_hist', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 913,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(FV,dtype=np.float32)\n",
    "# Y = np.array(lab,dtype=np.float32)\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train = min_max_scaler.fit(X[train_index])\n",
    "X_train_transform = min_max_scaler.transform(X[train_index])\n",
    "\n",
    "xg_reg.fit(X_train_transform, Y[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- *EVALUATION using Validation set* -----------------\n",
      "Mean Squared Error: 2.955\n",
      "Concordance index: 0.667999\n",
      "aupr: 0.692\n"
     ]
    }
   ],
   "source": [
    "#######################Validation SET Evaluation######################### \n",
    "\n",
    "X_valid_transform = min_max_scaler.transform(X[valid_index])\n",
    "predictedY= xg_reg.predict(X_valid_transform)\n",
    "\n",
    "print(\"----------------- *EVALUATION using Validation set* -----------------\")\n",
    "# Evaluation Metrics (RMSE, CI, and AUPR)\n",
    "print('Mean Squared Error: %.3f' % mean_squared_error(Y[valid_index], predictedY))\n",
    "print('Concordance index: %3f' % get_cindex(Y[valid_index], predictedY))\n",
    "print('aupr: %.3f' % average_AUPRC(Y[valid_index], predictedY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- *Test Set EVALUATION* -----------------\n",
      "Mean Squared Error: 3.036\n",
      "Concordance index: 0.662360\n",
      "aupr: 0.673\n"
     ]
    }
   ],
   "source": [
    "########################################## Test SET Evaluation\n",
    "\n",
    "print(\"----------------- *Test Set EVALUATION* -----------------\")\n",
    "X_test_transform = min_max_scaler.transform(X[test_index])\n",
    "# Make predictions using the testing set\n",
    "predictedY_test= xg_reg.predict(X_test_transform)\n",
    "\n",
    "# Evaluation Metrics (RMSE, CI, and AUPR)\n",
    "print('Mean Squared Error: %.3f' % mean_squared_error(Y[test_index], predictedY_test))\n",
    "print('Concordance index: %3f' % get_cindex(Y[test_index], predictedY_test))\n",
    "print('aupr: %.3f' % average_AUPRC(Y[test_index], predictedY_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Affinity2Vec Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(alpha=5, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.8, eval_metric='rmse',\n",
       "             gamma=0, gpu_id=1, importance_type='gain',\n",
       "             interaction_constraints='', learning_rate=0.01, max_delta_step=0,\n",
       "             max_depth=11, min_child_weight=6, missing=nan,\n",
       "             monotone_constraints='(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)',\n",
       "             n_estimators=529, n_jobs=-1, num_parallel_tree=1, random_state=10,\n",
       "             reg_alpha=5, reg_lambda=1, scale_pos_weight=1, seed=10,\n",
       "             subsample=1, tree_method='gpu_hist', validate_parameters=1,\n",
       "             verbosity=None)"
      ]
     },
     "execution_count": 916,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((FV,concatenateFV), axis=1)\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(booster = 'gbtree', objective ='reg:squarederror', eval_metric = 'rmse',\n",
    "                    colsample_bytree = 0.8, learning_rate = 0.01, max_depth = 11, scale_pos_weight = 1, gamma=0,\n",
    "                     alpha = 5,n_estimators = 529, tree_method='gpu_hist',min_child_weight =6, \n",
    "                    seed=10, gpu_id=1, n_jobs=-1) #better & faster\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train = min_max_scaler.fit(X[train_index])\n",
    "X_train_transform = min_max_scaler.transform(X[train_index])\n",
    "\n",
    "xg_reg.fit(X_train_transform, Y[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- *EVALUATION using Validation set* -----------------\n",
      "Mean Squared Error: 2.221\n",
      "Concordance index: 0.730162\n",
      "aupr: 0.802\n"
     ]
    }
   ],
   "source": [
    "#######################Validation SET Evaluation######################### \n",
    "\n",
    "X_valid_transform = min_max_scaler.transform(X[valid_index])\n",
    "predictedY= xg_reg.predict(X_valid_transform)\n",
    "\n",
    "print(\"----------------- *EVALUATION using Validation set* -----------------\")\n",
    "# Evaluation Metrics (RMSE, CI, and AUPR)\n",
    "print('Mean Squared Error: %.3f' % mean_squared_error(Y[valid_index], predictedY))\n",
    "print('Concordance index: %3f' % get_cindex(Y[valid_index], predictedY))\n",
    "print('aupr: %.3f' % average_AUPRC(Y[valid_index], predictedY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- *Test Set EVALUATION* -----------------\n",
      "Mean Squared Error: 2.286\n",
      "Concordance index: 0.722227\n",
      "aupr: 0.784\n"
     ]
    }
   ],
   "source": [
    "########################################## Test SET Evaluation\n",
    "\n",
    "print(\"----------------- *Test Set EVALUATION* -----------------\")\n",
    "X_test_transform = min_max_scaler.transform(X[test_index])\n",
    "# Make predictions using the testing set\n",
    "predictedY_test= xg_reg.predict(X_test_transform)\n",
    "\n",
    "# Evaluation Metrics (RMSE, CI, and AUPR)\n",
    "print('Mean Squared Error: %.3f' % mean_squared_error(Y[test_index], predictedY_test))\n",
    "print('Concordance index: %3f' % get_cindex(Y[test_index], predictedY_test))\n",
    "print('aupr: %.3f' % average_AUPRC(Y[test_index], predictedY_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ----------------------------------- END of the CODE -----------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
